* Machine Learning
  https://developers.google.com/machine-learning/crash-course/

** Description

A supervised machine learning systems learn how to combine input to produce
useful predictions on never-before-seen data.

** Terminology

- *label* is the thing we are predicting (sometimes also called a *target*);
- *feature* is an input variable;
- *example*
  - is a particular instance of data;
  - is categorized as labed examples and unlabled examples;
- *model*
  - defines the relationship between features and labels;
  - labled examples are used to *train* the model;
  - it is important to check how reliable your data is;
  - the labels applied to some examples may not be trusted;
  - a trained model is used to predict the label on unlabled examples;
- *training* means creating or *learning* a model;
- *inference* means applying a trained model to unlabled examples;
- a *regression* model predicts continuous values;
- a *classification* predicts discrete values;
- *linear regression* is a method for finding the straight line or hyperplane
  that best fits a set of points;
- *loss* is how well our line is doing at predicting any given example, is the
  penality for a bad prediction;

** Learning from data

- there are a lot of complex ways to learn from data, but starting simple will
  open the door to broadly useful methods;
- the equation generally has a shape like `y = wx + b`
- a convenient loss function for regression is *squared error* (also called *L2
  loss*), that calculates the square of the difference between prediction and
  true value: (label-prediction)^2 or (y-y')^2;
- Defining L2 loss on a data set we sum the loss of examples in the training set
  (sometimes is useful to define the mean square error - MSE, to calculate MSE,
  we sum up all the squared losses and then divide by the number of examples);
- by convention in machine learning, we write the equation for a model in the
  format y' = b + wx where:
  y' is the predicted layer;
  b is the bias;
  w is a vector representing the weight of the features;
  x is a vector representing the features;
- training a model means define good values for all the weights and bias from
  labeled examples.
- in *supervised learning*, a machine learning algorithm examines many examples
  to build a model (finding a set weights and biases) minimizing the loss
  (having low loss, on average, across all examples);

** Reducing loss

- *hyperparameters* are the configurations used to tune how the model is trained.
- *derivative* of the loss function tells us how loss changes for a given
  example and gives us an efficient way to update model parameters.
- we repeatedly take small steps in the direction that minimizes loss, we call
  these *gradient steps* and the strategy is called *gradient descent*.
- when the data comes in, we compute the gradient of the loss function on that
  data and the negative gradient tells us in which direction to update the model
  parameters in order to reduce the loss, we get a new version of the model and
  now we can compute the gradient and repeat. We can keep taking gradient steps
  in that direction until we have reached a point in which we have passed the
  local minimum in which the negative gradient will tell us to go back in the
  direction that we came from.
- The *learning rate* dictates how large a step should be in the direction of
  negative gradient.
- If the learning rate is too small we may require a lot of computation to
  reach the minimum.
- If the learning rate is too large we may overshoot the local minimum and even
  reach a point in which the loss is bigger than before and also cause the model
  to diverge (in that case we should try to decrease the learning rate by an
  order of magnitude).
- In practice, finding a "perfect" (or near-perfect) learning rate is not
  essential for successfull model training. The goal is to find a learning rate
  large enough that gradient descent converges efficiently, but not so large
  that it never converges.
- in gradient descent algorithm, it matter were we start.
- many machine learning problems are not "convex" (neural networks, for
  example), which means that their curves is not shaped like a "bowl", they are
  shaped more like an "egg crate" where there are many possible minimum values,
  so there initialization does matter.
- neural networks:
  - non convex
  - more than one minimum
  - strong dependency on initial values
- compute gradient on small data samples (not over the entire data set on each
  step) works well, getting a new random sample on every step.
- *stocastic gradient descent*: one example at a time.
- *mini-batch gradient descent*: batches of 10-1000 in which loss and gradients
  are averaged over the batch.
- in practice, generally, we use the strategy of mini-batch gradient descent,
  the intermediate solution between use one example only and the entire data
  set.
- this interative learning might remind the "hot and cold" kid's game for
  finding a hidden object.
- interative strategies are prevalent in machine learning, primarily because
  they scale so well to large data sets.
- for linear regression problems, it turns out that the starting values of "b"
  and "w" are not important, so we could pick random values or trivial values
  like zero, for example. In each step the machine learning system will examine
  the value of the loss function and generates new values of "b" and "w",
  repeating this process until the algorithm discover the values that results
  the lowest possible loss.
- usually we iterate until overall loss stops changing or changing extremely
  slowly. When that happens we say that the model has *converged*.
- the gradient of loss is equal to the derivative (slope) of the curve, and
  tells which way is "warmer" or "colder" when  there are multiple weights.
- the gradient is a vector of partial derivatives with respect to the weights.
- the gradient always points in the direction of steepest increase in the loss
  function. The gradient descent algorithm takes a step in the direction of the
  negative gradient in order to reduce loss as quickly as possible.
- to determine the next point, the gradient descent algorithm adds some fraction
  of the gradient's magnitude to the previous point.
- when performing gradient descent, web generalize the process to tune all the
  model parameters simultaneously, calculating the gradients with respect to
  both "b" and "w" and updating their respective values.
- the gradient vector has both a direction and a magnitude.
- gradient descent algorithms multiply the gradient by the learning rate (step
  size) to define the next point. If the gradient magnitude is 2.5 and the
  learning rate is 0.01, the next point will be 0.025 away from the previous
  one.
- the goldilocks value is related to how flat the loss function is. If the
  gradient is small a larger learning rate can be used, which compensates the
  small grand and results in a larger step size.

** First steps with TensorFlow

- *TensorFlow* is a generic *graph-based computational framework* that can be
  used to build machine learning models.
- It offers *high-level APIs* (like tf.estimator) to specity predefined
  architectures, such as *linear regressors* and *neural networks*.
- It offers also a *lower-level API*.
- Useful tools that generaly are use with it:
  - *SciKit* (metrics)
  - *Pandas* (mainly *DataFrames* and *Series*)
  - *NumPy* (*math operations* like log, sin, etc)
  - *MatPlotLib* (2D charts)
- Examble of a linear regression with TensorFlow:
  https://colab.research.google.com/notebooks/mlcc/first_steps_with_tensor_flow.ipynb#scrollTo=AZWF67uv0HTG
  - import libs
  - load dataset
  - randomize data
  - define features
  - define targets
  - configure linear regressor (the model)
  - define input function (the trainer)
  - train the model
  - evaluate results
    - comparing mean squared errors (MSE and RMSE) with MAX-MIN target values
    - comparing predictions and target values (pd.DataSet.describe)
  - plot chart with a sample data selection
  - test combinations of settings (learning rate, steps, batch size) to improve
    the results
- Some tips/rules to test combinations of settings of the training (despite the effects are data dependent)
  - training error should steadily decrease
  - if the training has not converged, try running it for longer
  - if the training error decreases too slowly, increasing the learning rate may help it decrease faster
    - but sometimes the opposite may happen if the learning rate is too high
  - if the training error varies wildly, try decreasing the learning rate
    - lower learning rate plus larger number of steps or larger batch size is often a good combination
  - very small batch sizes can also cause instability, first try larger values like 100 or 1000, and decrease until you see degradation
- Other tips that can help to improve the model
  - try a *synthetic feature*
  - remove outliers (histograms can help to visualize distributions)

** Generalization

- Just fitting the training data is not enough to be doing machine learning, we
  are interested in generalization
- we need avoid *over-fit* the peculiarities of the training data to be able to
  classify new previously unseen data successfully
- an over-fit model gets a low loss during training but does a poor job
  predicting new data
- there is a whole field around this, called *generalization theory*
- the first machine learning theoretician was William Ockham (from the 14th
  century)
- *i.i.d* = independently and identically draws
- we can use the *test set* methodology to ensure generalization
  - one way is to divide your data set into two subsets:
    - training set: a subset to train a model.
    - test set: a subset to test the model.
  - good performance on the test set is a useful indicator of good performance
    on the new data in general, assuming that:
    - we are always pulling from the same distribution
    - the distribution is stationary (its behaviour does not change over time)
    - the test set is large enough
    - we don't cheat by using the same test set over and over
- it's important for us to be aware of test set methodology assumptions and also
  to pay careful attention to the metrics, any time we know that these
  assumptions might be violated.
- in practice, we sometimes violate these assumptions

** Training and test sets

- one thing we can do is we can divide a large data set into two smaller sets,
  and use one for training and one for test
- these are two things in tension when we reason about how large do we make our
  different splits
  - the larger we make our training set, the better model we're going to be able
    to learn
  - the larger we make our test set, the better we'll be able to have confidence
    in our evaluation metrics, we'll have tighter confidence intervals
- if we have a very large data set (billions of examples), we might be able to
  hold out 10-15% for tests and have strong confidence intervals there
- if we have a very small data set, then we might need to do something more
  sophisticated like cross-validation
- make sure that your test set meets the following two conditions:
  - is large enough to yield statistically meaningful results
  - is representative of the data set as a whole (don't pick different
    characteristics than the training set).
- *never train on test data* (pay attention to cut duplicated entries)
- if you "magically" have 100% accuracy on your test data, double check that you
  have not accidentally trained on your test data
- if the model is not over-fitting the training data, it will have a training
  loss simmilar to the test loss
- a good goal is to have a small delta between training loss and test loss

** Validation set

- The more often we evaluate on a given test set, the more we are at risk for
  implicitly overfitting to that one test set
- using only two partitions may be insufficient when doing many rounds of
  hyperparameter tuning
- we can greatly reduce your chances of overfitting by partitioning the data set
  into the three subsets (training set, *validation set* and test set)
- having a validation set we can maintain our test data completely out of the
  training and hyperparameters validation, doing also the final test with a
  protected test set
- in this improved workflow:
  - pick the model that does best on the validation set
  - double-check that model against the test set
- the test set shows if we are overfitting the validation set
- analyzing the california housing data set, we could select the first 12k
  examples as training data and the last 5k examples as training set, but this
  strategy would have some issues:
  - the target can have different ranges on training and validation sets if it
    is sorted by year
  - many targets may have different ranges on training and validation sets if
    they have different locations (latitude and longitude)
  - median_income is on a scale from about 3 to 15, but it is not at all clear
    what this scale refers, it is not documented anywhere
  - the rooms_per_person feature is generally on a sane scale, with a 75th
    percentile value of about 2. But there are some very large values, like 18
    or 55, which may show some amount of outliers or corruption in the data
- to improve our analysis and avoid issues related to data sorting, is a good
  idea to randomize the data set before select the training and validation data
