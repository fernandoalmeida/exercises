* The Natural Language Processing Workshop

** 1 - Introduction to Natural Language Processing

   *Natural Language* is a mutually agreed upon set of protocols involving words/sounds that we
   use to communicate with each other.

   *NLP* can be defined as a field of computer science that is concerned with enabling computer
   algorithms to understand, analyze, and generate natural languages.

   *Text analytics* is the method of extracting meaningful insights and answering questions from
   text data *without getting into the semantics* of the language.

   *NLP helps us in understanding the semantics* and the underlying meaning of text.

   NLP works at different *levels*:
   - *Morphological level*: This level deals with understanding word structure and word information.
   - *Lexical level*: This level deals with understanding the part of speech of the word.
   - *Syntactic level*: This level deals with understanding the syntactic analysis of a sentence, or
     parsing a sentence.
   - *Semantic level*: This level deals with understanding the actual meaning of a sentence.
   - *Discourse level*: This level deals with understanding the meaning of a sentence beyond just
     the sentence level, that is, considering the context.
   - *Pragmatic level*: This level deals with using real-world knowledge to understand the sentence.


   NLP can be broadly categorized into two *types*: Natural Language *Understanding* (NLU) and
   Natural Language *Generation* (NLG).

   NLP applies a wide range of *tasks*:
   - *Tokenizing*: splitting a sentence into its constituent parts
   - *Parts-of-speech tagging*: refers to the process of tagging words within sentences as noun,
     verb, adjective, etc.
   - *Stop-word removal*: the most frequently occurring words in any language (articles,
     prepositions, etc)
   - *Normalization*: different variations of text get converted into a standard form.
     There are various ways of normalizing text, such as:
     - *Spelling correction*: one of the most *important* tasks in any NLP (it's time-consuming):
       - *Identify*: the misspelled word.
       - *Suggest*: the correctly spelled word.
     - *Stemming*: convert words into their base forms.
     - *Lemmatization*: convert words to their base grammatical form.

   *Named Entity Recognition (NER)* is the process of extracting important entities, such as
   person names, place names, and organization names, from some given text.

   *Word sense disambiguation* is the process of mapping a word to the sense that it should carry.

   *Sentence boundary detection* is the method of detecting where one sentence ends and another
    begins.

   We can divide an NLP project into several *phases*. These phases are completed in a particular
   order:
   1. *data collection*: use existing data, collect from online repositories, crawling web.
   2. *data processing*: clean the collected data to ensure effectiveness and accuracy:
     - Converting all the text data to lowercase.
     - Stop word removal.
     - Text normalization.
     - Stemming and lemmatization.
   3. *feature extraction*: convert text data into its equivalent numerical form
   4. *model development*: models generally statistical, machine learning based, deep learning
      based, or reinforcement learning based.
   5. *model assessment*: evaluate the performance of the model by comparing its precision,
       recall, and accuracy to others.
   6. *model deployment*: put into production, integrated to existing system or new products.

** 2 - Feature extraction methods

   There are two main ways to categorize data:
   - *by structure*:
     - *structured*: the most organized form of data (ex: databases, CSV).
     - *semi-structured*: can be transformed into a table (ex: XML, HTML).
     - *unstructured*: difficult to comprehend without loss information (ex: logs, images).
   - *by content*:
     - *text*: written sentences.
     - *image*: pictures.
     - *audio*: voice recordings, music.
     - *video*: a continuous series of images coupled with audio.

   Data cleaning has the objective of extracting meaningful portions from data by eliminating
   unnecessary details (noisy) that can negatively impact the accuracy of the results.

   Machine learning algorithms do not understand textual data directly, it need to be represented
   as numerical form or vectors.

   Features can be classified into two categories:
   - *General features*: do not depend on the content of the text (statistical calculations).
   - *Specific features*: dependent on the meaning of the text (represent the semantics).

   The *Bag of Words (BoW)* model is one of the most popular feature extraction methods:
   - The list of all the words (among all documents) is generated.
   - The document is represented in terms of the presence or absence of all words.

   According to *Zipf's law*, the number of times a word occurs in a corpus is inversely
   proportional to its rank.

   *Term Frequency-Inverse Document Frequency (TFIDF)* is another represention of text as vector,
   it gives more weightage to less frequent (rare) than more frequent (common) words.

   There are different techniques for finding the similarity between texts:
   - *Cosine similarity*: calculating the cosine of the angle between them.
   - *Jaccard similarity*: Calculated as the ratio of the number of terms that are common between
     two text documents to the total number of unique terms present in those texts. Only works on
     BoW vectors.

   The *Lesk algorithm* can be used for words with ambiguous meanings.

   A *word cloud* is a text visualization format in which the size of the word is related to its
   frequency.

   There are many ways of visualizing texts, some popular are:
   - *dependency parse tree*: A data structure for mapping dependencies among sentences. For
     example, the word "help" depends on "the one who helps" and "the ones who are helped".
   - *named entities*: highlight named entities by using different colors.

** 3 - Developing a text classifier

   *Machine learning* is the scientific study of algorithms and statistical models that computer
   systems use to perform tasks using patterns and inference instead of explicit instructions.

   Machine learning is categorized as:
   - *Unsupervised learning* algorithms learn patterns within data that is not labeled, labels
   (supervisors) are absent. This category is divided as:
     - *clustering*: combining objects into groups.
     - *association*: obtain groups of items that occur together frequently.

   *Hierarchical clustering* groups similar objects to create a cluster based on a *dendrogram*, a
   representation in the form of a tree.

   *K-means clustering* is a two phase algorithm (assignment and update) that groups objects into
   "K" clusters, applying the *elbow method* to discover the optimal "K".

   *Supervised learning* need labeled data, it predicts values by analyzing features (independent
   variables) of the data provided.

   *Classification* algorithms are those that learn patterns from a known dataset to determine
   classes of unknown datasets.

   Example of classification algorithms:
   - *Logistic Regression*: used for probabilistic classification, the dependent variable (the
     outcome) is binary.
   - *Naive Bayes*: another kind of probabilistic classifier.
   - *k-nearest neighbor*: used for regression and classification, applies the concept of
     *homophily* (people who have similar interests stay close).

   Formally, *regression analysis* refers to the process of learning a mapping function, which
   relates features or predictors (inputs) to the dependent variable (output).

   There are various types of regression:
   - *univariate*: just one dependent variable
   - *multivariate*: two or more dependent variables
   - *simple*: only one predictor or target variable
   - *multiple*: more than one predictor variable
   - *linear*
   - *non-linear*
   - *polynomial regression*
   - *stepwise regression*
   - *ridge regression*
   - *lasso regression*
   - *elastic net regression*

   Linear regression in the base algorithm for all the different types of regression.

   There are several algorithms that have both classification and regression forms.

   *Tree-based* methods have high accuracy and model linear and non-linear relationships. Its
    better than linear regression for handling categorical variables.
   - *decision tree*:
   - *random forest*: a forest is a collection of different types of trees, we use several
     decision trees for prediction.

   In a random forest, an individual tree's vote impacts the final decision.

   Random forest can be used for classification and regression tasks.

   Random forest uses a sampling technique called *bagging*, which *prevents overfitting*.

   Building a random forest often takes a huge amount of time and memory.

   There are various other tree-based algorithms, such as *gradient boosting machines (GBM)* and
   *extreme gradient boosting (XGBoost)*.

   *Boosting* works by combining "weak" models into a single more accurate model.

   XGBoost is an enhanced version of GBM. It is portable, distributed, fast (written in C++) and
   allows to store data on disk instead memory (which is useful for very large datasets).

   *Sampling* is the process of creating a subset from a given set of instances, it s necessary
   when creating models for imbalanced datasets.

   There are different kinds of sampling methods:
   - *Simple random sampling*: each instance has an equal probability of being selected.
   - *Stratified sampling*: the original set is divided into parts called "strata", based on given
     criteria. Random samples are chosen from each of these "strata" according to its proportion.
   - *Multi-Stage Sampling*: perform stratified sampling at each and every stage.

   A *text classifier* is a machine learning model that is capable of labeling texts based on
   their content.

   *Feature engineering* is the art of extracting new features from existing ones.

   *Correlation* refers to the statistical relationship between two variables.

   Regression models, including logistic regression, are unable to perform well when correlation
   between features exists.

   Features with correlation beyond a certain threshold need to be removed.

   The most widely used correlation statistic is Pearson correlation.

   Sometimes, the TFIDF or Bag-of-Words representation is too big and doesn't fit in memory. In
   this cases, we can reduce its dimensionality using *Principal Component Analysis (PCA)* method.

   When deciding on a unsupervised model, if we have a predefined number of clusters in mind, we
   go for k-means, otherwise, we opt for hierarchical clustering.

   When deciding on a supervised model, if the outcome is continuous and numeric, we use
   regression. If it is discrete or categorical, we use classification.

   When we need a simple classification model, Naive Bayes algorithm is a good choice.

   When we need to achieve higher accuracy, more complex tree-based methods such as decision trees
   and random forests are good options.

   When we want the probability of the occurrence, we use logistic regression.

   Once a model is ready, it is necessary to evaluate its performance using methods like:
   - *confusion matrix*: a crosstab between actual and predicted values.
   - *accuracy*: ratio of correctly classified instances to the total number of instances.
   - *precision and recall*: evaluates the relevancy of the result items and expected items.
   - *F1 Score*: the harmonic mean of precision and recall.
   - *Receiver Operating Characteristic (ROC) Curve*: a chart based on *True Positive Rate (TPR)*
     and *False Positive Rate (FPR)*.
   - *Root Mean Square Error (RMSE)*:  mainly used for evaluating regression models.
   - *Maximum Absolute Percentage Error (MAPE)*: another way to evaluate a regression models*.

   The *pickle* lib makes use of binary protocols to save and load Python objects.

   *joblib* provide efficient ways to save large Python objects (using *pickle*).

** Links / references

- https://github.com/nltk/nltk
- https://github.com/sloria/TextBlob
- https://keras.io/
- https://scikit-learn.org/
- https://spacy.io/
- https://matplotlib.org/
- https://github.com/amueller/word_cloud
