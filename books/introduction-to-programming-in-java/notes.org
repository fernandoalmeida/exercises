* Chapter 4 - Algorithms and Data Structures

** Analysis of Algorithms

   To study the cost of running programs we use a *scientific method* (techniques
     commonly accepted, universally used to develop knowledge about natural
     world):

     - *Observe* some features of the natural world;
     - *Hypothesize* a model that is consistent with the observations;
     - *Predict* events using the hypothesis;
     - *Verify* the predictions by making further observations;
     - *Validate* by repeating until the hypothesis and observations agree.

   The *experiment* that we design must be *reproducible* and *falsifiable*.

   We need to make *quantitative measurements* of the running time of the
   programs, normally *related to input size or input value*, usually we are
   satisfied with *approximate estimates*.

   The *mathematical analysis* is determined by two primary factors:

     - The *cost* of executing each statement;
     - The *frequency* of execution of each statement.

   We use *tilde notation* to develop approximate expression.
   *~ f(N)* represents any quantity that divided by f(N) approaches 1 as N grows.
   *g(N) ~ f(N)* indicates that g(N) / f(N) approaches 1 as N grows.

   For a great many programs the running time satisfies the relationship T(N) ~
   c f(N) where c is a constant and f(N) a function known as *order of growth*,
   commonly f(N) is a function such as:

     *1* = *constant*
       does not depend on the input size;
     *log N* = *logarithmic*
       slightly slower as N grows;
       whenever N double, the running time increases by a constant;
     *N* = *linear*
       optimal if you need process N inputs;
       whenever N double, then so does the running time.
     *N log N* = *linearithmic*
       scales to huge problems;
       whenever N double, the running time is a little more than doubles;
     *N^2* = *quadratic*
       practical for use only on relatively small problems;
       whenever N doubles, the running time increases fourfold;
     *N^3* = *cubic*
       practical for use only on small problems;
       whenever N doubles, the running time increases eightfold;
     *2^N* = *exponential*
       is not usually appropriate for practical use;
       whenever N doubles, the running time squares;
     *N!* = *factorial*
       worse than exponential;
       Whenever N increases by 1, the running time increases by a factor of N;

   We need also to measure the memory usage. For primitive types is not
   difficult to estimate memory consumption, we can count the number of
   variables and weight them by number of bytes according to their type:

     boolean = 1
     char = 2
     int / float = 4
     long / double = 8

     object reference = 4

     char[] = 16 + 2N
     string = 40 + 2N

     int[] = 16 + 4N
     int[][] = ~ 4N^2
     double[][] = ~ 8N^2

   To determine the memory usage of an object, we add the amount of memory used
   by each instance variable to the overhead associated with each object,
   typically 8 bytes.

   An impossibly slow program is almost as useless as an incorrect one.

** Sorting and Searching

   One reason that sorting is so useful is that it is much easier to search
   something in a sorted list than an unsorted one.

   *Binary search* is often called *bisection search* because we bisect the
   interval at each stage.

   Searching an element in a sorted array using binary search we start at the
   entry in the middle, eliminating either the subarray before or after the
   current entry (~ log N), whereas using a *brute force* solution would be start
   at the beginning examining each entry until the final (~ N).

   *Insertion sort* is a brute force sorting algorithm based on a simple method
    similar to arrange hands of playing cards (~ N^2).

   *Mergesort* is a faster sorting method that uses a *devide-and-conquer*
   approach with a *recursive* strategy that to mergesort an array we devide it
   in two halves, sort them independently and merge the results (~ N log N).

   Problems related to sorting and searching:

     Frequency counts:
       - Zipf's law says that the frequency of the ith most frequent word of M
         distinct word is proportional to 1/i
         (https://en.wikipedia.org/wiki/Zipf%27s_law)
     Longest repeated substring:
       - computer-assisted music analysis
       - cryptography
       - data compression

** Stacks and Queues

   *Stacks* and *Queues* are two closely-related data types for manipulating
   arbitrarily large collection of items defining `push/insert` and `pop/remove`
   operations.

   The removal operation in a queue remove the oldest item (FIFO),
   in a stack it remove the newest item (LIFO).

   Implementations:

     Array
       + push and pop operations in constant time (~ 1);
       - maybe (in Java, for example) we need to set a maximum size and use space proportional to that
         maximum;
     Linked list
       + push and pop operation in constant time (~ 1);
       + is a recursive data structure allowing arbitrarily large collections;
       + the space used is always proportional to the number of items in the
         collection;
     Array double
       + alternative to linked list;
       + dynamically adjust the size of the array (if necessary) to add new
         items;
       - double the size of the array, by creating a new array of twice the
         size, copying the stack items to the new array, and resetting the
         reference to the new array;
     Parameterized data types
       + can insert objects of any type;
       - when we pop it, client must cast to the appropriate type (error prone);
       - contradicts "ask don't touch" principle;

   It is common to implement an `Iterator/Enumerable` interface for traversing
   operation.

   Queues applications:
     - serving shared resources (printer, disk, CPU);
     - transferring data asynchronously between two processes (IO buffers as
       pipes, File IO, sockets);
     - interrupt handling;

   Stack applications:
     - Parsing;
     - Function calls;

** Symbol Tables

   A *symbol table* is a data type used to associate values and keys pairs using
   put and get operations.

   The keys generally are immutable, comparable (to allow ordering) and is not
   permitted to use `null` as key or value
     contains = get(key) !â‰ƒ null
     remove = put(key, null)

   Applications:

     - DNS;
     - Indexing;

   Implementations:

     Array (binary search)
       - Maintains two parallel arrays of keys and values keeping them in
         key-sorted order.
       - get = ~ log N;
       - put = ~ N;
     Linked list
        - get = ~ N;
	- put = ~ N;
     Binary Search Tree
       - recursive;
       - in worst case the *BST* is totally unbalanced and get/put are ~ N;
       - using a variation of BST known as *red-black tree* the get/put in the
         worst case is ~ log N.
       - allows extended operations (min, max, range)
     Set
       - is a symbol table with no values;
       - could use BST to implement it;
       - direct implementation is simpler and clearer than using BST;

** Graphs

   A mathematical model, comprised of a set of vertices and a set of edges,
   used to study pairing among entities where edges represents a connection
   between two vertices;

   *small-world phenomenon* is a graph property also known as *six degree of
    separation*

   Applications:
     - Telecom;
     - Circuits;
     - Social networks;
     - Internet hyperlinks;

   Given two vertices in a graph, a *path* is a sequence of edges connecting
   them.

   *Breadth-first search* (*BFS*) is an algorithm for traversing or searching
   tree or graph data structures. It starts at the tree root (or some arbitrary
   node of a graph, sometimes referred to as a 'search key' and explores the
   neighbor nodes first, before moving to the next level neighbors.
   (https://en.wikipedia.org/wiki/Breadth-first_search)

   One of the classic applications of *shortest-paths* algorithms is to find the
   degrees of separation of individuals in social networks.
